# Story 4.1: Real-time AI Question Generation

Status: ready-for-dev

<!-- Note: Validation is optional. Run validate-create-story for quality check before dev-story. -->

## Story

As a **User**,
I want **the probing questions to be generated by a real AI model in real-time**,
so that **they are tailored to the specific Job Description I entered**.

## Acceptance Criteria

### ✅ Happy Paths

1.  **Real-time Generation Trigger**
    - **Given** I save a new job
    - **When** the `generate_probing_questions` background task executes
    - **Then** the system calls the configured AI Provider (Gemini/Ollama) via `AIExtractorDep`
    - **And** the mock data generator is bypassed.

2.  **Context-Aware Questions**
    - **Given** a Job Description with specific technical requirements (e.g., "Python, FastAPI")
    - **When** the AI generates questions
    - **Then** the questions reference these specific technologies (e.g., "How have you used FastAPI...?")
    - **And** the output aligns with the `ProbingQuestion` schema.

3.  **Database Persistence**
    - **Given** the AI response is received successfully
    - **When** the data is parsed
    - **Then** `ProbingQuestion` records are created in the database linked to the Job
    - **And** the Job status updates to "Ready".

### ❌ Sad Paths (Network & API)

4.  **AI Service Unavailable**
    - **Given** the AI provider API is down or rate-limited
    - **When** the generation attempts
    - **Then** the system logs the error with a Romanised Hindi message
    - **And** the Job remains in "Draft" state (or retries if configured)
    - **And** the UI reflects the failure or pending state.

5.  **Invalid AI Response**
    - **Given** the AI returns malformed JSON or non-compliant text
    - **When** the parser validation fails
    - **Then** the system falls back to a safe state (e.g., logs error, does not crash)
    - **And** no partial/corrupted questions are saved.

### ⚠️ Edge Cases

6.  **Empty/Short JD**
    - **Given** a Job Description with very little text (e.g., "Hring Devs")
    - **When** the AI receives this context
    - **Then** it generates generic behavioural questions instead of hallucinating technical ones
    - **And** the system still produces a valid checklist.

## Tasks / Subtasks

- [ ] **Backend: AI Service Implementation**
  - [ ] Update `generate_probing_questions` in `jobs.py` (or service layer) to use `AIExtractorDep`.
  - [ ] Implement `generate_questions_prompt` in `prompts.py` (Create if missing).
  - [ ] Implement response parsing logic (JSON to Pydantic).
  - [ ] Remove/Disable `seed_questions.py` reliance for production flow.
- [ ] **Backend: Provider Enhancements**
  - [ ] Ensure `GeminiProvider` handles structured output (JSON mode or schema enforcement).
  - [ ] Add error handling for API timeouts/quota limits.
- [ ] **Configuration**
  - [ ] Verify `settings.AI_PROVIDER` and API keys are loaded correctly.
- [ ] **Verification**
  - [ ] Unit test with `Mock` AI provider to verify pipeline.
  - [ ] Integration test with real Gemini API (optional/manual).
  - [ ] Verify "Short JD" edge case.

## Technical Notes

- **Architecture:** Use `app.services.ai_factory.get_ai_provider` for dependency injection.
- **Prompt Engineering:** Ensure the prompt enforces a STRICT JSON schema matching `ProbingQuestion`.
- **Performance:** Use `async` calls to avoid blocking the main thread during AI generation.
- **Romanised Hindi:** Add comments to all new functions.

### File List

- `backend/app/api/route_utils.py` (or where background task lives)
- `backend/app/services/ai_extractor.py`
- `backend/app/services/gemini_provider.py`
- `backend/app/services/prompts.py` (New)
- `backend/app/core/config.py`

### Dev Agent Record

### Change Log
